{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNIwe5N7s0e_"
   },
   "source": [
    "# Real-world Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BG63Tpg8ep_"
   },
   "source": [
    "In this project, you will apply the skills you acquired in the course to gather and wrangle real-world data with two datasets of your choice.\n",
    "\n",
    "You will retrieve and extract the data, assess the data programmatically and visually, accross elements of data quality and structure, and implement a cleaning strategy for the data. You will then store the updated data into your selected database/data store, combine the data, and answer a research question with the datasets.\n",
    "\n",
    "Throughout the process, you are expected to:\n",
    "\n",
    "1. Explain your decisions towards methods used for gathering, assessing, cleaning, storing, and answering the research question\n",
    "2. Write code comments so your code is more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDYDkH-Zs7Nn"
   },
   "source": [
    "## 1. Gather data\n",
    "\n",
    "In this section, you will extract data using two different data gathering methods and combine the data. Use at least two different types of data-gathering methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbN7z7rcuqpO"
   },
   "source": [
    "### **1.1.** Problem Statement\n",
    "In 2-4 sentences, explain the kind of problem you want to look at and the datasets you will be wrangling for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gi6swhjSYqu2"
   },
   "source": [
    "I would like to perform an analysis on the UFO sightings and how they correlate to the birth totals/rates in the United States. To do this, I will need to capture data regarding births from the United States governement, the CDC Natality Birth data reports or CDC Wonder web tool. The UFO data will need to be either obtained from a dataset combined previously by another or capture directly from the NUFORC website. I decided to use the CDC Wonder web tool and web scraping from NUFORC, but I did notice that the granularity of the data is uneven. The UFO data goes down to the city level while the CDC data only goes down to the county level. I will use a another data source, or two, to capture the FIPS code data so that I may associate the cities provided with the county they reside in. The main source I will be obtaining this data from is the Department of Transportations API connection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AQfBAdUypMm"
   },
   "source": [
    "### **1.2.** Gather at least two datasets using two different data gathering methods\n",
    "\n",
    "List of data gathering methods:\n",
    "\n",
    "- Download data manually\n",
    "- Programmatically downloading files\n",
    "- Gather data by accessing APIs\n",
    "- Gather and extract data from HTML files using BeautifulSoup\n",
    "- Extract data from a SQL database\n",
    "\n",
    "Each dataset must have at least two variables, and have greater than 500 data samples within each dataset.\n",
    "\n",
    "For each dataset, briefly describe why you picked the dataset and the gathering method (2-3 full sentences), including the names and significance of the variables in the dataset. Show your work (e.g., if using an API to download the data, please include a snippet of your code). \n",
    "\n",
    "Load the dataset programmtically into this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e6gS0wL1KTu"
   },
   "source": [
    "#### **Dataset 1**: FIPS Data\n",
    "\n",
    "Type: *API :: CSV Extraction to Dataframe* \n",
    "\n",
    "Method: Data was captured from through the use of the Department of Transportation's API tool. The API required that the SodaPy Socrata python library be used to access, authenticate, and download the data. The data was downloaded using a loop to capture the data in chunks due to an allowed limit restriction for each query result. The data was then combined and into a single dataframe. Prior to performing the cleaning, I do export the data as a pickle and csv file as a localized backup of the download.  \n",
    "\n",
    "**Beginning Dataset variables**:\n",
    "\n",
    "*   *state_name* :: Text name of the State\n",
    "*   *county_name* :: Text name of the County\n",
    "*   *city_name* :: Text name of the City\n",
    "*   *state_code* :: Two Character abbreviation of the State\n",
    "*   *state_fipcode* :: FIP Code designation of the state, two digit number\n",
    "*   *county_code* :: FIP Code assigned to the County, \"C\" + Three Digit Number \n",
    "*   *county_fipcode* :: Full FIP Code designation down to county granularity, this is a five digit code comprised of the two digit state code and the three digit code from the county\n",
    "*   *city_code* :: FIP Code assigned to the City, this is a four digit number. \n",
    "*   *city_fipcode* :: Full FIP Code designation down to city granularity. This is a nine digit code comprised of the five digit fip code above with the four digit city code on the end.\n",
    "\n",
    "\n",
    "\n",
    "The original data is converted into three dataset that will be utilized during the ETL process. The first of these is the FIPS Data Main, which will be the main cleaning and conversion of the original source data into it's cleaned format. The second dataset is a table that will be used for vectorized searching and matching of the state name or state code to it's respective fipcode. \n",
    "\n",
    "**FIPS Data Main Variables**:\n",
    "*   *state_name* :: Text name of the State\n",
    "*   *state_code* :: Two Character abbreviation of the State\n",
    "*   *state_fipcode* :: FIP Code designation of the state, two digit number\n",
    "*   *county_name* :: Text name of the County\n",
    "*   *county_fipcode* :: FIP Code assigned to the County, \"C\" + Three Digit Number \n",
    "*   *city_name* :: Text name of the City\n",
    "*   *city_fipcode* :: FIP Code assigned to the City, this is a four digit number.\n",
    "*   *fips_five* :: The five digit fip code designating the state and county.\n",
    "*   *fips_nine* :: The nine digit fip code designating the state, county, and city.\n",
    "*   *multi_county_flag* :: A boolean column to indicate if a city exists in multiple counties.\n",
    "*   *county_count* :: If the city exists in multiple counties, the number of counties will show here, otherwise the number is one. \n",
    "*   *county_rank* :: If the city exists in multiple counties, the counties are ranked in asceding order based on the county code.\n",
    "\n",
    "**State Lookup Table Variables**:\n",
    "*   *state_lookup* :: Text name or two-character abbreviation (state code) for the state. \n",
    "*   *fips* :: State FIP Code\n",
    "\n",
    "This table has the index created on the state_lookup column to create fast, lite matching and searching. \n",
    "\n",
    "**County City Lookup Table Variables**:\n",
    "*   *state_fipcode* :: Contains the two digit fipcode of the state. \n",
    "*   *county_lookup* :: Contains the text name of the county. \n",
    "*   *county_fipcode* :: Contains the County FIP code, C###. \n",
    "*   *city_lookup* :: Contains the city text name. \n",
    "*   *city_fipcode* :: Contains the four digit numerical fip code for the city. \n",
    "\n",
    "This table works with the same principle as the state_lookup table but just includes the county and city data as well. This table is replaced later with another updated version during the process. The table is also fed into the fuzzy query module used when cleaning the city data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoUjq1tPzz7P"
   },
   "source": [
    "#### Dataset 2 :: CDC Data\n",
    "\n",
    "Type: *Text Files*\n",
    "\n",
    "Method: Web scraping/Automation using the Python module, Selenium, with the CDC Wonder web tool. The data was generated using the official CDC Wonder Web tool to perform automated execution of the web form's options and trigger a download of the text file results. There are three total database the data is pulled from. The data from the 1995-2002 datasets has a different format than the data from the remaining two database from 2003 to 2023. \n",
    "\n",
    "Dataset variables:\n",
    "\n",
    "**1995-2002 Dataset:** \n",
    "\n",
    "*   *Notes* :: Column was used to designate special rows or changes in the data structure. \n",
    "*   *Year* :: Column contained the integer/text four digit year value. \n",
    "*   *Year Code* :: Column contained the Codified version of the year value, which was the same four digit year as before.\n",
    "*   *State* :: Text name of the State \n",
    "*   *State Code* :: 2 digit FIP code designation for the State\n",
    "*   *County* :: Text name of the County \n",
    "*   *County Code* :: Five digit FIPS code designation for the State and County. \n",
    "*   *Births* :: Total number of births that occurred for that row entry, which was an aggregation of that year, state, county's birth\n",
    "\n",
    "Although these two were not necessarily variables, they were apart of the original structure of the data. \n",
    "*   *Footer* :: At the end of each data table, a footer of text information was given to provide important information regarding changes, alterations, updates to the dataset's details. \n",
    "*   *Totals* :: These rows appeared throughout the dataset, typically a section of data had concluded such as the completion of a state's counties and it provided an aggregated total of that groups births. These total rows occurred after the end of each level of grouping/data per say, state, year, overall.\n",
    "\n",
    "**2003-2023 Dataset:** \n",
    "\n",
    "*   *Notes* :: Column was used to designate special rows or changes in the data structure. \n",
    "*   *Year* :: Column contained the integer/text four digit year value. \n",
    "*   *Year Code* :: Column contained the Codified version of the year value, which was the same four digit year as before.\n",
    "*   *Month* :: Text name of the Month\n",
    "*   *Month Code* :: Column contained a codified version of the month's value, which would be the numerical representation of the month. \n",
    "*   *State* :: Text name of the State \n",
    "*   *State Code* :: 2 digit FIP code designation for the State\n",
    "*   *County* :: Text name of the County \n",
    "*   *County Code* :: Five digit FIPS code designation for the State and County. \n",
    "*   *Births* :: Total number of births that occurred for that row entry, which was an aggregation of that year, state, county's birth\n",
    "\n",
    "Although these two were not necessarily variables, they were apart of the original structure of the data. \n",
    "*   *Footer* :: At the end of each data table, a footer of text information was given to provide important information regarding changes, alterations, updates to the dataset's details. \n",
    "*   *Totals* :: These rows appeared throughout the dataset, typically a section of data had concluded such as the completion of a state's counties and it provided an aggregated total of that groups births. These total rows occurred after the end of each level of grouping/data per say, state, month, year, and overall. These totals different from the ones above because now the data granularity includes months. \n",
    "\n",
    "**##For all databases:** By the end of the cleaning process, the CDC data will contain the following variables: \n",
    "\n",
    "*   *year_code* :: Column contained the Codified version of the year value, which was the same four digit year as before.\n",
    "*   *month_code* :: Column contained a codified version of the month's value, which would be the numerical representation of the month.  \n",
    "*   *state_fipcode* :: 2 digit FIP code designation for the State\n",
    "*   *county_fipcode* :: The three digit county FIP Code, C###. \n",
    "*   *fips_five* :: Five digit FIPS code designating the state and county. a\n",
    "*   *Births* :: Total number of births that occurred during that month for that year, state, and county. \n",
    "\n",
    "The footer files that were originally apart of the data are cut off the table data and saved to their own folder inside the documents directory, /docs/cdc_data_footers. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 3 :: UFO Data\n",
    "\n",
    "Type: *CSV file*\n",
    "\n",
    "Method: Data was captured using two methods of web scraping, Beautiful Soup and Requests. The UFO sightings data was scraped from the NUFORC website using beautiful soup to capture the session token and then the request library would then take over performing requests to the web server's backend to obtain results in batches. The batches were saved to into a dataframe and exported every 10,000 items to prevent from creating too large of a file that the system could not handle it's saving. \n",
    "\n",
    "Dataset variables:\n",
    "\n",
    "*   *report_id* :: This column was originally a localized/partial link to the report's details page. I stripped the link text leaving only the numerical unique id value for the report. \n",
    "*   *occurred_on* :: This column allowed the user to input a date and time into the field regarding when they observed the sighting. \n",
    "*   *city* :: This column allowed allowed the user to input text indicating the city the sighting was observed in.\n",
    "*   *state* :: This column allowed allowed the user to input text indicating the state the sighting was observed in. \n",
    "*   *country* :: This column allowed the user to input text to indicating the country the sighting was observed in. \n",
    "*   *shape* :: This column allowed the user to input text to describe the shape of the sighting.\n",
    "*   *summary* :: This column allowed the user to input text providing a summary of the sighting.\n",
    "*   *reported_on* :: This column appears to be a server side timestamp generated upon the incident being saved into the database by the user. \n",
    "*   *has_media* :: This column was boolean value to indicate if media (photo, video, etc) was uploaded with the sighting report\n",
    "*   *explanation* :: This column allowed the user to input text providing a possible explanation for the sighting\n",
    "*   *report_link* :: This column was not originally apart of the webserver's data. However, upon export of the data, I took the partial/localized link that was originally in the report id column and generated a direct link to the report's incident page.\n",
    "\n",
    "**By the end** of the dataset cleaning process. The dataset will have the following variables: \n",
    "\n",
    "*   *report_id* :: The orignal report_id generated during the extraction process. \n",
    "*   *year_code* :: The codified verison of the year, four digit year value. \n",
    "*   *month_code* :: The codified version of the month, numerical representation of the month. \n",
    "*   *state_fipcode* :: The two digit FIP code for the state\n",
    "*   *county_fipcode* :: The three digit FIP code for the county, C###. \n",
    "*   *city_fipcode* :: The four digit FIP code for the city. \n",
    "*   *fips_five* :: The five digit FIP code designating the state and county. \n",
    "*   *fips_nine* :: The nine digit FIP code designating the state, county, and city.\n",
    "\n",
    "The remaining data that was apart of the original extraction was removed and archived during the cleanup process. I made sure to include the report id on other columns data that was split off the from this data (shape, summary, etc) so that the records can be linked together easily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional data storing step: You may save your raw dataset files to the local data store before moving to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various methods utilized. \n",
    "\n",
    "The finalized cleaned data is stored in a SQL database, as well as in CSV and Pickle format inside the cleaned data directory. \n",
    "\n",
    "During the data extraction phase of the project, the data is downloaded and saved to local files to their own source folder inside the /data/raw directory. As mentioned above, the CDC Data downloads natively as a text file. The FIPS data is extracted and stored into a dataframe in memory, but to ensure that data loss does not occur, I export it as a pickle and csv file as a checkpoint. The UFO data is scrapped and saved in patcheds to CSV files as it downloads. \n",
    "\n",
    "Once the cleaning process begins, a copy of the data each sources data is transferred to their folder in the processing directory, /data/processed. The raw data that was downloaded/extracted is then archived through a zip utility and moved into its respective folder inside the archived directory, /data/archived. The raw folder is then emptied. From this point forward, all cleaning and processing of the data will be saved into it's source folder inside the processing directory, /data/processed. Throughout the cleaning process, checkpoints are created to create a restore point in the data. When this occurs, the data in the processing folder is typically then archived and moved to the archive folder and removed from the processing folder. There are some exceptions to this archiving, such as when the data is apart of a larger cleaning/transformation process, so the archive is held to the end to group like items. When the archive does occur, the data with a new export of the working data, thus creating a new checkpoint. Almost all checkpoints are a csv and pickle file export.  \n",
    "\n",
    "Once all cleaning has been completed, the data is uploaded into a database file which can be found in the database folder, /data/db. All processed data is archived to the archive folder and the processed data folders are emptied. Then a copy of the uploaded data from the database is exported into a dataframe and then exported as a checkpoint, csv and pickle file, into it's source folder in the cleaned data directory, /data/cleaned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwSWIVmotLgV"
   },
   "source": [
    "## 2. Assess data\n",
    "\n",
    "Assess the data according to data quality and tidiness metrics using the report below.\n",
    "\n",
    "List **two** data quality issues and **two** tidiness issues. Assess each data issue visually **and** programmatically, then briefly describe the issue you find.  **Make sure you include justifications for the methods you use for the assessment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adaK2iPNzVu4"
   },
   "source": [
    "### Quality Issue 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SpW59kh-zl8d"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qfcocStzsKg"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe programmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue and justification: *FILL IN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Be77N4I1AmE"
   },
   "source": [
    "### Quality Issue 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iMhHyiyLM2I3"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnviRCUI-bb7"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe programmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue and justification: *FILL IN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXhGiYyiwwKN"
   },
   "source": [
    "### Tidiness Issue 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fleC5rORI0Xl"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTuQw7Rbsio4"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe programmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue and justification: *FILL IN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ffMoRGSwzYj"
   },
   "source": [
    "### Tidiness Issue 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUpeoqokw5Qt"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8JK4DoXxtFA"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Inspecting the dataframe programmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue and justification: *FILL IN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6gmLnBttpCh"
   },
   "source": [
    "## 3. Clean data\n",
    "Clean the data to solve the 4 issues corresponding to data quality and tidiness found in the assessing step. **Make sure you include justifications for your cleaning decisions.**\n",
    "\n",
    "After the cleaning for each issue, please use **either** the visually or programatical method to validate the cleaning was succesful.\n",
    "\n",
    "At this stage, you are also expected to remove variables that are unnecessary for your analysis and combine your datasets. Depending on your datasets, you may choose to perform variable combination and elimination before or after the cleaning stage. Your dataset must have **at least** 4 variables after combining the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL IN - Make copies of the datasets to ensure the raw dataframes \n",
    "# are not impacted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmFhN52Yyn3l"
   },
   "source": [
    "### **Quality Issue 1: FILL IN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9UejDWrNMW4a"
   },
   "outputs": [],
   "source": [
    "# FILL IN - Apply the cleaning strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUBee-LPytkv"
   },
   "outputs": [],
   "source": [
    "# FILL IN - Validate the cleaning was successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justification: *FILL IN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_DAUbJrymBL"
   },
   "source": [
    "### **Quality Issue 2: FILL IN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Yfb-Yu5MTuE"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Apply the cleaning strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ionB2sRaMUmY"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Validate the cleaning was successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justification: *FILL IN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIUrrfSNyOPR"
   },
   "source": [
    "### **Tidiness Issue 1: FILL IN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fib0zAm333bn"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Apply the cleaning strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhrnUGY_Nk8B"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Validate the cleaning was successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justification: *FILL IN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o51Bt8kwyTzk"
   },
   "source": [
    "### **Tidiness Issue 2: FILL IN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zW8O5yx4Y9O"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Apply the cleaning strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "q6I_Sr7lxXi5"
   },
   "outputs": [],
   "source": [
    "#FILL IN - Validate the cleaning was successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justification: *FILL IN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Remove unnecessary variables and combine datasets**\n",
    "\n",
    "Depending on the datasets, you can also peform the combination before the cleaning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILL IN - Remove unnecessary variables and combine datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F42urHuzttjF"
   },
   "source": [
    "## 4. Update your data store\n",
    "Update your local database/data store with the cleaned data, following best practices for storing your cleaned data:\n",
    "\n",
    "- Must maintain different instances / versions of data (raw and cleaned data)\n",
    "- Must name the dataset files informatively\n",
    "- Ensure both the raw and cleaned data is saved to your database/data store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "V3uay7EJUV_L"
   },
   "outputs": [],
   "source": [
    "#FILL IN - saving data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGy_yddGtzhM"
   },
   "source": [
    "## 5. Answer the research question\n",
    "\n",
    "### **5.1:** Define and answer the research question \n",
    "Going back to the problem statement in step 1, use the cleaned data to answer the question you raised. Produce **at least** two visualizations using the cleaned data and explain how they help you answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjedE4s4ZkEd"
   },
   "source": [
    "*Research question:* Is there a correlation between UFO Sightings and Births within the United States?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Lkw3rW9kZmOm"
   },
   "outputs": [],
   "source": [
    "#Visual 1 - FILL IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer to research question:* FILL IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6fdK_8ZGZm9R"
   },
   "outputs": [],
   "source": [
    "#Visual 2 - FILL IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5RgvMGUZoHn"
   },
   "source": [
    "*Answer to research question:* FILL IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ezWXXZVj-TP"
   },
   "source": [
    "### **5.2:** Reflection\n",
    "In 2-4 sentences, if you had more time to complete the project, what actions would you take? For example, which data quality and structural issues would you look into further, and what research questions would you further explore?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XB3RBDG5kFe1"
   },
   "source": [
    "*Answer:* FILL IN"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
