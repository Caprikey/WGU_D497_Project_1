{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4b283d6-a3d6-46c6-9047-187516e3e0de",
   "metadata": {},
   "source": [
    "# Order Of Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931e4f1e-30af-44e2-962c-e9bc3c3b6571",
   "metadata": {},
   "source": [
    "### **IMPORTANT / READ FIRST**: \n",
    "\n",
    "#### How to: \n",
    "\n",
    "Starting at Step 1: \n",
    "\n",
    "To run each of these notebook, please perform one of the following: \n",
    "\n",
    "- Use the \"Run All\" option under the Run menu in the tool bar.\n",
    "- Open the notebook and go to the last cell in the notebook and select it. Then perform the \"Run All Above Selected Cell\" option from the Run Menu in the toolbar.\n",
    "- Open the notebook, select the first cell, and then run each cell individually in order. This is best done by using the keyboard short-cut \"Shift + Enter\". This will run the select cell and then select the next cell in the notebook.\n",
    "\n",
    "At the end of every notebook, there is a markdown cell that will link you to the notebook for the next step or you may use the link to the landing to comeback to this landing page. \n",
    "\n",
    "# **DO NOT RUN OUT OF ORDER** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16bb90d-abc2-4e27-8bf8-742332b6ba01",
   "metadata": {},
   "source": [
    "### Step 0: Checking for Required \"folder_paths.json\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d30b6c2e-41ce-4150-a18e-f36c9fabba54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists\n",
      "Proceed with Step 1\n"
     ]
    }
   ],
   "source": [
    "# Import OS \n",
    "import os\n",
    "\n",
    "file_path = \"../data/folder_paths.json\"  # Replace with the actual file path\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    print(\"File exists\")\n",
    "    print(\"Proceed with Step 1\")\n",
    "else:\n",
    "    print(\"File does not exist\")\n",
    "    print(\"Running folder_manager.py file from d497_helpers module\")\n",
    "    %run \"../d497_helpers/folder_manager.py\"\n",
    "    print(\"Proceed with Step 1\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2056859e-cad7-4789-a43d-63addde303cb",
   "metadata": {},
   "source": [
    "### Step 1: [Data Extraction - FIPS Data](data_extraction_fips.ipynb) \n",
    "\n",
    "FIPS Data is obtained from the United States Department of Transportation through API access. The data is captrued into a dataframe then exported as a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a34bfa-17e9-41e8-9d7b-15f396185dd1",
   "metadata": {},
   "source": [
    "### Step 2: [Data Extraction - UFO Data](data_extraction_ufo.ipynb)\n",
    "\n",
    "UFO Sighting Data is captured from the NUFORC website through two methods of web scraping, Beautiful Soup and Requests. Beautiful soup is utilized to capture the session token from the web site, and then requests makes multiple calls to the website's backend to capture the data in loop. The data is saved to captured into a dataframe then exported as a csv file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff405192-94af-43b8-aff9-cce72d766049",
   "metadata": {},
   "source": [
    "### Step 3: [CDC Data Extraction - CDC Data](data_extraction_cdc.ipynb)\n",
    "\n",
    "CDC Data is captured from the CDC Wonder web portal tool using web automation tool, Selenium, to automate the generation of text file downloads. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae37cc48-7b79-4278-936f-e8b5dfc8a305",
   "metadata": {},
   "source": [
    "### Step 4: [Data Cleaning - FIPS Data](data_cleaning_fips_main.ipynb)\n",
    "\n",
    "FIPS data is prepared for load into the SQL database here. The data is checked for null values, U.S. States, and transformed to fit a more unified format for this project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dceb10d-dc68-453e-b5c7-10134c1229f4",
   "metadata": {},
   "source": [
    "### Step 5: [Data Load - FIPS Data](data_load_fips.ipynb)\n",
    "\n",
    "The FIPS data is loaded into a SQL database here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be920218-b965-43ef-aae9-5ff700756f34",
   "metadata": {},
   "source": [
    "### Step 6: [Data Transformation - State Lookup Data](data_transformation_state_lookup.ipynb)\n",
    "\n",
    "The State Loopkup table is generated here. FIPs Data from the SQL databased is exported and transformed into a table that will be utilzied for vectorized searching, which will allow for faster and more efficicent matching of the State Name or State Abbreviation to the State's FIP Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07990bb3-e0b3-43a3-9f81-082cd9b7315f",
   "metadata": {},
   "source": [
    "### Step 7: [Data Transformation - County City Lookup Data](data_transformation_city_lookup.ipynb)\n",
    "\n",
    "The County and City Lookup table is made here. The method is the same as above with the state lookup table, but this time focusing on creating a table that will allow for the matching of City names to the County. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d4a47b-ebcc-4454-a578-c1687b8f16c6",
   "metadata": {},
   "source": [
    "### Step 8: [Data Cleaning - CDC Data](data_cleaning_cdc_main.ipynb)\n",
    "\n",
    "The CDC Data is cleaned here to be prepared for import into the SQL database. \n",
    "\n",
    "The CDC data undergoes several main cleanup operations. The first of this would be the splitting of the informational footer from the original text files. Next the 'total' rows are removed from each file as well. The third main change that occurs is for the years from 1995 to 2002. The database that contains this information did not store the data in monthly totals but yearly totals. To correct this uniformity, the monthly data is imputed by splitting the yearly into monthly evenly. \n",
    "\n",
    "Once the imputing is finished the data is then recombined and converted down to a more uniformed format. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b438ea-13a2-4a23-b117-c63965779a0e",
   "metadata": {},
   "source": [
    "### Step 9: [Data Load - CDC Data](data_load_cdc.ipynb)\n",
    "\n",
    "CDC Data is loaded into the SQL database here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974ddf71-711f-4a89-b0a6-ad67dc51fee8",
   "metadata": {},
   "source": [
    "### Step 10: [Data Transformation - Removal of Unspecified Counties from CDC Data ](data_transformation_cdc_data_unspecified_county_removal.ipynb)\n",
    "\n",
    "The initial CDC data contained records for births that were not linked to a specific county, these were originally stored as Unspecified Counties. This notebook removes those by evenly distributes the data into each county for the state. The data is exported from the datbase, transformed, and imported back into the database as a new table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32c6b7d-56b9-4737-ad26-3a7414107e4b",
   "metadata": {},
   "source": [
    "### Step 11: [Data Cleaning - UFO](data_cleaning_ufo_main.ipynb)\n",
    "\n",
    "The UFO data cleaning is performed here. This notebook is extensive. \n",
    "\n",
    "The data is stripped of any additioanl whitespaces, new lines, etc. From here the data is then split into the columns that will be utilized for this report and those that will not be. To prevent from needed to perform all the extraction again, the data that is not utilized is zipped and archived for potential future use. Once the data has been stripped and split, it is combined into from it's multiple file into a single file. \n",
    "\n",
    "The data then is then transformed over a series of steps into a more unified format to be used for analysis. The majority of these transformations occurs when working with the city data. The CDC Data is only goes down to County level granularity. While the UFO data allowed for users to input City, State, and Country, all of which was not restrictive in their entry. Due to these differences in granularity and strictness of data accuracy. Multiple passes are necessary to begin cleaning up and matching the data to valid locations. The first of these passes was to remove non-United States countries. Next, the states are matched using the State Lookup table made previously. After this the values in the city column have to be cleaned and matched the county where they reside to equalize the granularity of the data. The city data is stripped of all rows that contain special characters. Next the dataset is merged with County City Lookup table created before. Duplicates are removed for entires that have multiple counties for each city. Then non-matched entries are then checked row by row against the County and City Lookup table for individually. The rows that are then left after this that are not matched are run through a third pass to match the row value to the county and city lookup table using a fuzzy search tool called RapidFuzz. The remaining non-matched files are manually reviewed and updated or filtered out of the database due to invalid data. After the main data base been matched, the special character data filtered previously is then ran through the same fuzzy match tool to attempt to match the entires against the county and city loopkup table. Finally the data is joined back together. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42c8c68-9955-4b3a-81dd-30db224f74ef",
   "metadata": {},
   "source": [
    "### Step 12: [Data Load - UFO](data_load_ufo.ipynb)\n",
    "\n",
    "UFO Data is loaded into the database here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46654bb-18fc-4241-a42e-9eb005edd28b",
   "metadata": {},
   "source": [
    "### Step 13: [Data Transformation - UFO Data Aggregation](data_transformation_ufo_data_aggregation.ipynb)\n",
    "\n",
    "The UFO data is exported and aggregated from single entry occurrences into a total groups for each year, month, state, and county like the CDC Data is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692650a9-9819-45bb-86ae-a65370f3f8f4",
   "metadata": {},
   "source": [
    "### Step 14: [Data Analysis](data_analysis_main.ipynb)\n",
    "\n",
    "Data Analysis occurs here, please run all the fields and then go to the project start file. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f32904-2d96-4eb5-9460-dfdabbbfb172",
   "metadata": {},
   "source": [
    "### Step 15: [Data Wrangling Project Starter Notebook](data_wrangling_project_starter.ipynb)\n",
    "\n",
    "Answer all the academic required questions and provides the response to my analysis. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
